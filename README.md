# GRPO Training Framework

åŸºäº Qwen2.5-7B-Instruct çš„ GRPOï¼ˆGrouped Relative Policy Optimizationï¼‰è®­ç»ƒæ¡†æ¶ã€‚

## ç‰¹æ€§

- ğŸš€ **é«˜æ•ˆè®­ç»ƒ**: æ”¯æŒ LoRA/QLoRA å¾®è°ƒï¼Œé€‚é…æ¶ˆè´¹çº§ GPU
- ğŸ“Š **ç»„å†…ç›¸å¯¹å¥–åŠ±**: æ— éœ€ Critic ç½‘ç»œï¼Œé™ä½è®­ç»ƒæˆæœ¬
- ğŸ¯ **å¤šä»»åŠ¡æ”¯æŒ**: äº²å±å…³ç³»ã€å¤šè·³æ¨ç†ã€é—®ç­”ç­‰åœºæ™¯
- ğŸ”§ **æ˜“äºä½¿ç”¨**: ç®€æ´çš„é…ç½®å’Œè®­ç»ƒæ¥å£

## å®‰è£…

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. ä¿®æ”¹æ¨¡å‹è·¯å¾„

åœ¨ `scripts/train_grpo.py` ä¸­ä¿®æ”¹æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š

```python
--model_path "/Users/xry/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B-Instruct"
```

### 2. è¿è¡Œè®­ç»ƒ

```bash
cd /Users/xry/Desktop/python/projects/Agent-R1
python scripts/train_grpo.py \
    --model_path "/Users/xry/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B-Instruct" \
    --task_type kinship \
    --group_size 3 \
    --learning_rate 5e-5 \
    --batch_size 2 \
    --epochs 5 \
    --use_lora
```

### 3. ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®

åˆ›å»ºæ•°æ®æ–‡ä»¶å¹¶ä¿®æ”¹è®­ç»ƒè„šæœ¬ï¼š

```python
from src.grpo import GRPOTrainer

# å‡†å¤‡æ‚¨çš„æ•°æ®
queries = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
ground_truths = ["ç­”æ¡ˆ1", "ç­”æ¡ˆ2", "ç­”æ¡ˆ3"]

# åˆ›å»ºæ•°æ®é›†
dataset = SimpleDataset(queries, ground_truths)

# è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
from src.grpo import KinshipRewardFunction
reward_function = KinshipRewardFunction(
    path_length_penalty=0.1,
    correct_answer_bonus=1.0
)

# åˆå§‹åŒ–è®­ç»ƒå™¨å¹¶è®­ç»ƒ
trainer = GRPOTrainer(
    model=model,
    tokenizer=tokenizer,
    grpo_config={...},
    training_config={...},
    reward_function=reward_function
)
```

## é…ç½®å‚æ•°

### GRPO é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `group_size` | 3 | æ¯ç»„è½¨è¿¹æ•°é‡ |
| `clip_epsilon` | 0.1 | PPO è£å‰ªç³»æ•° |
| `kl_coeff` | 0.05 | KL æ•£åº¦ç³»æ•° |
| `learning_rate` | 5e-5 | å­¦ä¹ ç‡ |
| `normalize_reward` | True | æ˜¯å¦æ ‡å‡†åŒ–å¥–åŠ± |

### LoRA é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `lora_r` | 16 | LoRA ç§© |
| `lora_alpha` | 32 | LoRA ç¼©æ”¾å› å­ |
| `lora_dropout` | 0.05 | Dropout æ¯”ç‡ |

## é¡¹ç›®ç»“æ„

```
Agent-R1/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_grpo.py          # è®­ç»ƒä¸»è„šæœ¬
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ grpo/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core.py            # GRPO æ ¸å¿ƒç®—æ³•
â”‚   â”‚   â”œâ”€â”€ reward.py          # å¥–åŠ±å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ generator.py       # è½¨è¿¹ç”Ÿæˆå™¨
â”‚   â”‚   â””â”€â”€ trainer.py         # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_utils.py     # æ¨¡å‹å·¥å…·
â”‚   â””â”€â”€ config.py              # é…ç½®ç®¡ç†
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
â”œâ”€â”€ outputs/                    # è¾“å‡ºç›®å½•
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## å‚è€ƒæ–‡çŒ®

- **GRPO**: Grouped Relative Policy Optimization
- **PPO**: Proximal Policy Optimization
- **LoRA**: Low-Rank Adaptation

## è®¸å¯è¯

MIT License
