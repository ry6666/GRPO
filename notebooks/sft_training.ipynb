{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ SFT LoRA è®­ç»ƒ - äº²å±å…³ç³»ä»»åŠ¡\n",
    "\n",
    "ä½¿ç”¨ M4 èŠ¯ç‰‡è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒè®­ç»ƒï¼ŒåŒ…å«ï¼š\n",
    "- æ•°æ®åŠ è½½ä¸å¢å¼º\n",
    "- æ¨¡å‹åŠ è½½ï¼ˆLoRA + float16ï¼‰\n",
    "- è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–\n",
    "- æŸå¤±æ›²çº¿ç»˜åˆ¶\n",
    "- æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ M4 Apple Silicon è®­ç»ƒç¯å¢ƒæ£€æµ‹\n",
      "============================================================\n",
      "\n",
      "ğŸ“± MPS å¯ç”¨: True\n",
      "ğŸ“± MPS å·²æ„å»º: True\n",
      "ğŸ”µ CUDA å¯ç”¨: False\n",
      "ğŸ“¦ PyTorch ç‰ˆæœ¬: 2.10.0\n",
      "ğŸ Python ç‰ˆæœ¬: 3.11.14\n",
      "\n",
      "âœ… ä½¿ç”¨è®¾å¤‡: mps\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥è¿è¡Œç¯å¢ƒ\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ M4 Apple Silicon è®­ç»ƒç¯å¢ƒæ£€æµ‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "mps_built = torch.backends.mps.is_built()\n",
    "\n",
    "print(f\"\\nğŸ“± MPS å¯ç”¨: {mps_available}\")\n",
    "print(f\"ğŸ“± MPS å·²æ„å»º: {mps_built}\")\n",
    "print(f\"ğŸ”µ CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"ğŸ“¦ PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"ğŸ Python ç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "device = torch.device(\"mps\" if mps_available else \"cpu\")\n",
    "print(f\"\\nâœ… ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     AutoTokenizer,\n\u001b[32m     15\u001b[39m     AutoModelForCausalLM,\n\u001b[32m     16\u001b[39m     DataCollatorForSeq2Seq,\n\u001b[32m     17\u001b[39m     get_linear_schedule_with_warmup\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, TaskType\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path(\u001b[34;43m__file__\u001b[39;49m).parent.parent))\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_lora_config\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkinship_augment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_augmented_data\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent))\n",
    "\n",
    "from src.sft import create_lora_config\n",
    "from src.data.kinship_augment import load_augmented_data\n",
    "\n",
    "print(\"âœ… ä¾èµ–å¯¼å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M4 ä¼˜åŒ–é…ç½®\n",
    "config = {\n",
    "    \"model_path\": \"/Users/xry/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B-Instruct\",\n",
    "    \"train_data_path\": \"./dataset/augmented/train.json\",\n",
    "    \"test_data_path\": \"./dataset/augmented/test.json\",\n",
    "    \"output_dir\": \"./outputs/sft_kinship\",\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 1,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"max_length\": 512,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"torch_dtype\": torch.float16,\n",
    "    \"device\": \"mps\"\n",
    "}\n",
    "\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“‹ è®­ç»ƒé…ç½®:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½å¢å¼ºåçš„æ•°æ®\n",
    "train_data, test_data = load_augmented_data(\n",
    "    config[\"train_data_path\"],\n",
    "    config[\"test_data_path\"]\n",
    ")\n",
    "\n",
    "queries = [item['query'] for item in train_data]\n",
    "answers = [item['answer'] for item in train_data]\n",
    "\n",
    "test_queries = [item['query'] for item in test_data]\n",
    "test_answers = [item['answer'] for item in test_data]\n",
    "\n",
    "print(f\"\\nğŸ“Š æ•°æ®è§„æ¨¡:\")\n",
    "print(f\"  è®­ç»ƒé›†: {len(queries)} æ¡\")\n",
    "print(f\"  æµ‹è¯•é›†: {len(test_queries)} æ¡\")\n",
    "\n",
    "print(f\"\\nğŸ“ ç¤ºä¾‹æ•°æ®:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Q: {queries[i]}\")\n",
    "    print(f\"  A: {answers[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config[\"model_path\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config[\"model_path\"],\n",
    "    torch_dtype=config[\"torch_dtype\"],\n",
    "    device_map=None,\n",
    "    trust_remote_code=True\n",
    ").to(config[\"device\"])\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "print(f\"  æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. é…ç½® LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½® LoRA\n",
    "lora_config = create_lora_config(\n",
    "    r=config[\"lora_r\"],\n",
    "    alpha=config[\"lora_alpha\"],\n",
    "    dropout=config[\"lora_dropout\"]\n",
    ")\n",
    "\n",
    "# åº”ç”¨ LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åˆ›å»ºæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, queries, answers, tokenizer, max_length=512):\n",
    "        self.queries = queries\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query = self.queries[idx]\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in kinship relationships.\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                conversation,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "        except:\n",
    "            text = f\"User: {query}\\nAssistant: {answer}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "train_dataset = SFTDataset(queries, answers, tokenizer, config[\"max_length\"])\n",
    "eval_dataset = SFTDataset(test_queries, test_answers, tokenizer, config[\"max_length\"])\n",
    "\n",
    "print(f\"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"  è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"  éªŒè¯é›†å¤§å°: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒæ­¥æ•°\n",
    "num_training_steps = len(train_loader) * config[\"epochs\"] // config[\"gradient_accumulation_steps\"]\n",
    "num_warmup_steps = int(num_training_steps * config[\"warmup_ratio\"])\n",
    "\n",
    "# è®¾ç½®ä¼˜åŒ–å™¨\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# è®¾ç½®è°ƒåº¦å™¨\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"âœ… ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è®¾ç½®å®Œæˆ\")\n",
    "print(f\"  æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps}\")\n",
    "print(f\"  é¢„çƒ­æ­¥æ•°: {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è®­ç»ƒå¾ªç¯ï¼ˆå«å¯è§†åŒ–ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æŸå¤±è®°å½•\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "learning_rates = []\n",
    "global_step = 0\n",
    "\n",
    "# åˆ›å»ºè¿›åº¦æ¡å’Œå›¾è¡¨\n",
    "from tqdm import tqdm\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "plt.ion()  # å¼€å¯äº¤äº’æ¨¡å¼\n",
    "\n",
    "print(\"ğŸš€ å¼€å§‹è®­ç»ƒ...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(config[\"device\"])\n",
    "        attention_mask = batch['attention_mask'].to(config[\"device\"])\n",
    "        labels = batch['labels'].to(config[\"device\"])\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / config[\"gradient_accumulation_steps\"]\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "            current_loss = loss.item() * config[\"gradient_accumulation_steps\"]\n",
    "            train_losses.append(current_loss)\n",
    "            learning_rates.append(scheduler.get_last_lr()[0])\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{current_loss:.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "            \n",
    "            # å®æ—¶æ›´æ–°å›¾è¡¨\n",
    "            if global_step % 10 == 0:\n",
    "                ax1.clear()\n",
    "                ax1.plot(train_losses, 'b-', alpha=0.6)\n",
    "                if len(train_losses) > 20:\n",
    "                    window = min(20, len(train_losses) // 5)\n",
    "                    smoothed = [sum(train_losses[max(0,i-window):i+1])/(i-max(0,i-window)+1) for i in range(len(train_losses))]\n",
    "                    ax1.plot(smoothed, 'r-', linewidth=2)\n",
    "                ax1.set_xlabel('Step')\n",
    "                ax1.set_ylabel('Loss')\n",
    "                ax1.set_title('Training Loss')\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                ax2.clear()\n",
    "                ax2.plot(learning_rates, 'g-')\n",
    "                ax2.set_xlabel('Step')\n",
    "                ax2.set_ylabel('Learning Rate')\n",
    "                ax2.set_title('Learning Rate')\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.pause(0.01)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # è¯„ä¼°\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch['input_ids'].to(config[\"device\"])\n",
    "            attention_mask = batch['attention_mask'].to(config[\"device\"])\n",
    "            labels = batch['labels'].to(config[\"device\"])\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            total_eval_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_eval_loss = total_eval_loss / len(eval_loader)\n",
    "    eval_losses.append(avg_eval_loss)\n",
    "    print(f\"Epoch {epoch+1} - Avg Eval Loss: {avg_eval_loss:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼\n",
    "print(\"âœ… è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç»˜åˆ¶æŸå¤±æ›²çº¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶æœ€ç»ˆæŸå¤±æ›²çº¿\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# è®­ç»ƒæŸå¤±\n",
    "axes[0].plot(train_losses, 'b-', alpha=0.6, label='Raw')\n",
    "if len(train_losses) > 20:\n",
    "    window = min(20, len(train_losses) // 5)\n",
    "    smoothed = [sum(train_losses[max(0,i-window):i+1])/(i-max(0,i-window)+1) for i in range(len(train_losses))]\n",
    "    axes[0].plot(smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# è¯„ä¼°æŸå¤±\n",
    "axes[1].plot(range(1, len(eval_losses)+1), eval_losses, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Evaluation Loss')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# å­¦ä¹ ç‡\n",
    "axes[2].plot(learning_rates, 'g-')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('SFT Training Results', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜å›¾ç‰‡\n",
    "loss_plot_path = f\"{config['output_dir']}/loss_curve.png\"\n",
    "plt.savefig(loss_plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"ğŸ’¾ æŸå¤±æ›²çº¿å·²ä¿å­˜: {loss_plot_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "save_path = f\"{config['output_dir']}/best_model\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒæ—¥å¿—\n",
    "training_log = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"eval_losses\": eval_losses,\n",
    "    \"learning_rates\": learning_rates,\n",
    "    \"config\": config,\n",
    "    \"final_train_loss\": train_losses[-1] if train_losses else None,\n",
    "    \"min_eval_loss\": min(eval_losses) if eval_losses else None\n",
    "}\n",
    "\n",
    "log_path = f\"{config['output_dir']}/training_log.json\"\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(training_log, f, indent=2)\n",
    "\n",
    "print(f\"ğŸ’¾ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ¨¡å‹\n",
    "model.eval()\n",
    "\n",
    "test_queries_small = test_queries[:10]\n",
    "test_answers_small = test_answers[:10]\n",
    "\n",
    "print(\"ğŸ§ª æ¨¡å‹æµ‹è¯•:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correct = 0\n",
    "for query, true_answer in zip(test_queries_small, test_answers_small):\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    except:\n",
    "        prompt = f\"User: {query}\\nAssistant:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(config[\"device\"])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    is_correct = response.strip() == true_answer.strip()\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    \n",
    "    print(f\"Q: {query}\")\n",
    "    print(f\"A: {response.strip()}\")\n",
    "    print(f\"âœ“\" if is_correct else \"âœ—\" + f\" (æ­£ç¡®: {true_answer})\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "accuracy = correct / len(test_queries_small) * 100\n",
    "print(f\"\\nğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.1f}% ({correct}/{len(test_queries_small)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. æ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ è®­ç»ƒæ€»ç»“\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ¯ æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}\" if train_losses else \"N/A\")\n",
    "print(f\"ğŸ¯ æœ€ä½è¯„ä¼°æŸå¤±: {min(eval_losses):.4f}\" if eval_losses else \"N/A\")\n",
    "print(f\"ğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.1f}%\")\n",
    "print(f\"ğŸ’¾ æ¨¡å‹ä¿å­˜ä½ç½®: {config['output_dir']}/best_model\")\n",
    "print(f\"ğŸ“ˆ æŸå¤±æ›²çº¿: {config['output_dir']}/loss_curve.png\")\n",
    "print(\"\\nâœ… SFT è®­ç»ƒå®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
