"""
æµ‹è¯• Ollama æœ¬åœ° Qwen æ¨¡å‹åœ¨äº²å±å…³ç³»ä»»åŠ¡ä¸Šçš„è¡¨ç°
è®­ç»ƒå‰åŸºå‡†æµ‹è¯•
"""

import os
import sys
import json
from pathlib import Path

try:
    import ollama
except ImportError:
    print("å®‰è£… ollama åº“...")
    os.system("uv pip install ollama -q")
    import ollama

from src.data.kinship_augment import load_augmented_data

def test_ollama_model(model_name="qwen2.5:7b"):
    print("=" * 70)
    print(f"ğŸ§ª Ollama {model_name} æ¨¡å‹æµ‹è¯• - äº²å±å…³ç³»ä»»åŠ¡")
    print("=" * 70)
    
    print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
    _, test_data = load_augmented_data(
        "./dataset/augmented/train.json",
        "./dataset/augmented/test.json"
    )
    
    test_queries = [item['query'] for item in test_data]
    test_answers = [item['answer'] for item in test_data]
    
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_queries)} æ¡\n")
    
    print("=" * 70)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
    print("=" * 70)
    
    correct = 0
    wrong = 0
    results = []
    
    for i, (query, true_answer) in enumerate(zip(test_queries[:30], test_answers[:30])):
        try:
            response = ollama.chat(
                model=model_name,
                messages=[
                    {"role": "system", "content": "è¯·ç®€çŸ­å›ç­”äº²å±å…³ç³»é—®é¢˜ï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸éœ€è¦è§£é‡Šã€‚"},
                    {"role": "user", "content": query}
                ],
                options={
                    "temperature": 0.1,
                    "num_predict": 30
                }
            )
            model_answer = response['message']['content'].strip()
        except Exception as e:
            print(f"API é”™è¯¯: {e}")
            model_answer = ""
        
        is_correct = model_answer == true_answer.strip()
        if is_correct:
            correct += 1
        else:
            wrong += 1
        
        results.append({
            'query': query,
            'true_answer': true_answer,
            'model_answer': model_answer,
            'correct': is_correct
        })
        
        status = "âœ…" if is_correct else "âŒ"
        print(f"[{status}] {i+1}/30: {query[:30]}...")
        print(f"   å›ç­”: {model_answer}")
        if not is_correct:
            print(f"   æ­£ç¡®: {true_answer}")
        print()
        
        if (i + 1) % 10 == 0:
            print(f"--- è¿›åº¦: {i+1}/30, æ­£ç¡®: {correct}, é”™è¯¯: {wrong} ---")
            print()
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    accuracy = correct / len(results) * 100
    print(f"\nğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.1f}% ({correct}/{len(results)})")
    print(f"âœ… æ­£ç¡®: {correct} æ¡")
    print(f"âŒ é”™è¯¯: {wrong} æ¡")
    
    print("\n" + "=" * 70)
    print("ğŸ“ æ ·æœ¬å±•ç¤º")
    print("=" * 70)
    
    for i, result in enumerate(results[:10]):
        status = "âœ…" if result['correct'] else "âŒ"
        print(f"\n[{status}] æ ·æœ¬ {i+1}")
        print(f"Q: {result['query']}")
        print(f"A: {result['model_answer']}")
        if not result['correct']:
            print(f"æ­£ç¡®: {result['true_answer']}")
        print("-" * 50)
    
    return results

if __name__ == "__main__":
    print("\næ£€æŸ¥ Ollama æœåŠ¡...")
    try:
        ollama.list()
        print("âœ… Ollama æœåŠ¡æ­£å¸¸\n")
    except Exception as e:
        print(f"âŒ Ollama æœåŠ¡å¼‚å¸¸: {e}")
        print("è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve")
        sys.exit(1)
    
    results = test_ollama_model("qwen2.5:7b")
    
    print("\n" + "=" * 70)
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    print("=" * 70)
    
    os.makedirs("./outputs", exist_ok=True)
    output_path = "./outputs/ollama_test_results.json"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
