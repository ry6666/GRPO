"""
æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹åœ¨äº²å±å…³ç³»ä»»åŠ¡ä¸Šçš„è¡¨ç°
è®­ç»ƒå‰åŸºå‡†æµ‹è¯•
"""

import os
import sys
from pathlib import Path

PROJECT_ROOT = Path.cwd()
sys.path.insert(0, str(PROJECT_ROOT))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from src.data.kinship_augment import load_augmented_data

def test_pretrained_model():
    print("=" * 70)
    print("ğŸ§ª é¢„è®­ç»ƒæ¨¡å‹åŸºå‡†æµ‹è¯• - äº²å±å…³ç³»ä»»åŠ¡")
    print("=" * 70)
    
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    model_path = "/Users/xry/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B-Instruct"
    
    print("\nğŸ”„ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map=None,
        trust_remote_code=True
    ).to(device)
    
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
    _, test_data = load_augmented_data(
        "./dataset/augmented/train.json",
        "./dataset/augmented/test.json"
    )
    
    test_queries = [item['query'] for item in test_data]
    test_answers = [item['answer'] for item in test_data]
    
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_queries)} æ¡\n")
    
    print("=" * 70)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
    print("=" * 70)
    
    correct = 0
    wrong = 0
    results = []
    
    for i, (query, true_answer) in enumerate(zip(test_queries[:50], test_answers[:50])):
        conversation = [
            {"role": "user", "content": query}
        ]
        
        try:
            prompt = tokenizer.apply_chat_template(
                conversation,
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            prompt = f"User: {query}\nAssistant:"
        
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=30,
                temperature=0.1,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        response = response.strip()
        
        is_correct = response == true_answer.strip()
        if is_correct:
            correct += 1
        else:
            wrong += 1
        
        results.append({
            'query': query,
            'true_answer': true_answer,
            'model_answer': response,
            'correct': is_correct
        })
        
        if (i + 1) % 10 == 0:
            print(f"è¿›åº¦: {i+1}/50, æ­£ç¡®: {correct}, é”™è¯¯: {wrong}")
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æµ‹è¯•ç»“æœ")
    print("=" * 70)
    
    accuracy = correct / 50 * 100
    print(f"\nğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.1f}% ({correct}/50)")
    print(f"âœ… æ­£ç¡®: {correct} æ¡")
    print(f"âŒ é”™è¯¯: {wrong} æ¡")
    
    print("\n" + "=" * 70)
    print("ğŸ“ æ ·æœ¬å±•ç¤º")
    print("=" * 70)
    
    for i, result in enumerate(results[:10]):
        status = "âœ…" if result['correct'] else "âŒ"
        print(f"\n[{status}] æ ·æœ¬ {i+1}")
        print(f"Q: {result['query']}")
        print(f"A: {result['model_answer']}")
        if not result['correct']:
            print(f"æ­£ç¡®: {result['true_answer']}")
        print("-" * 50)
    
    print("\n" + "=" * 70)
    print("ğŸ“ˆ é”™è¯¯åˆ†æ")
    print("=" * 70)
    
    wrong_results = [r for r in results if not r['correct']]
    if wrong_results:
        print(f"\né”™è¯¯ç±»å‹ç»Ÿè®¡:")
        
        wrong_types = {}
        for r in wrong_results:
            query_type = r['query'].split()[0] if r['query'].split() else "Unknown"
            if query_type not in wrong_types:
                wrong_types[query_type] = 0
            wrong_types[query_type] += 1
        
        for qtype, count in sorted(wrong_types.items(), key=lambda x: -x[1]):
            print(f"  {qtype}: {count} ä¸ªé”™è¯¯")
    
    return results

if __name__ == "__main__":
    results = test_pretrained_model()
    
    print("\n" + "=" * 70)
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    print("=" * 70)
    
    import json
    output_path = "./outputs/pretrained_test_results.json"
    os.makedirs("./outputs", exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
